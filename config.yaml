experiment_name      : "AES-EXP-18"
training_filename    : "final_train.csv"
data_dir             : "./input"
artifacts_dir        : "./output"
output_dir           : "${artifacts_dir}/${experiment_name}"
fold                 : 0
model_id             : "microsoft/deberta-v3-xsmall"
num_labels           : 6
wandb_log            : True
wandb_project_name   : "AES"
notes                : "Added mean pooling and freezed first 4 layers"
truncation           : True
max_length           : 1024
seed                 : 2024
full_fit             : False
notify_discord       : True
num_freez_layers     : 4
debug                : False
train_code_file      : "regression.py"

training_args :
  learning_rate               : 4.0e-5
  per_device_train_batch_size : 4
  per_device_eval_batch_size  : 8
  num_train_epochs            : 1
  weight_decay                : 0.01
  evaluation_strategy         : 'epoch'
  save_strategy               : 'no'
  push_to_hub                 : False
  warmup_ratio                : 0.1
  fp16                        : True
  lr_scheduler_type           : 'linear'
  logging_steps               : 100
  optim                       : 'adamw_torch'
  logging_first_step          : True
  # gradient_accumulation_steps : 2,
  # gradient_checkpointing      : True